{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Meta Bayes module\n",
                "\n",
                "This is a short preview of the Meta Bayes module.\n",
                "Meta Bayes strives to compute the optimal prior for a set of tasks.\n",
                "\n",
                "It relies on the penalized regression formulation of the inner PAC-Bayesian algorithm:\n",
                "\n",
                "$$\\hat\\theta =\\arg\\inf_\\theta \\tilde{S}_i(\\theta, \\theta_0) := \\pi(\\theta)[S_i] + \\lambda \\text{KL}(\\pi(\\theta), \\pi(\\theta_0))$$ \n",
                "\n",
                "Noting $A_i(\\theta_0)$ the solution of the task $i$ using prior $\\theta_0$, the meta score can be written as\n",
                "\n",
                "$$\\sum S_{i}^{meta}(\\theta_0) = \\tilde{S}_i(A_i(\\theta_0), \\theta_0).$$\n",
                "\n",
                "The meta learning algorithm uses gradient descent to minimize the meta_score, relying on \n",
                "\n",
                "$$\\nabla S_i^{meta} = \\lambda \\nabla F_i $$ \n",
                "where $F_i(\\theta) = \\text{KL}(\\pi(A_i(\\theta_0)), \\pi(\\theta))$.\n",
                "\n",
                "The meta learning tasks considered here are simple tasks where Gaussian conjugation occurs (quadratic risks, gaussian priors). This enables the algorithm to run more quickly in this demo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from surpbayes.meta_bayes import Task, MetaLearningEnv\n",
                "from surpbayes.proba import GaussianMap, TensorizedGaussianMap, BlockDiagGaussMap\n",
                "import numpy as np\n",
                "# Choose dimension/Number of tasks\n",
                "d = 4\n",
                "n_tasks = 100\n",
                "temperature = 0.1\n",
                "\n",
                "# Generate tasks\n",
                "def make_score(x):\n",
                "    def score(xs):\n",
                "        return ((x - xs) ** 2).sum(-1)\n",
                "\n",
                "    return score\n",
                "\n",
                "\n",
                "x0 = 0.5 + np.random.normal(0, 0.2, d)\n",
                "x_middles = x0 + np.random.normal(0, 0.1, (n_tasks, d))\n",
                "\n",
                "task_train = [\n",
                "    Task(make_score(x_mid), temperature=temperature, vectorized=True) for x_mid in x_middles\n",
                "]\n",
                "\n",
                "x_middles_test = x0 + np.random.normal(0, 0.1, (10, d))\n",
                "\n",
                "task_test = [\n",
                "    Task(make_score(x_mid), temperature=0.1, vectorized=True) for x_mid in x_middles_test\n",
                "]\n",
                "\n",
                "# Define distribution family\n",
                "proba_map = GaussianMap(d)\n",
                "\n",
                "# Define Meta Learning Environnement\n",
                "mlearn = MetaLearningEnv(\n",
                "    proba_map,\n",
                "    list_task=task_train,\n",
                "    # hyperparameters passed to training\n",
                "    per_step=50,\n",
                "    chain_length=2,\n",
                "    kl_max=100.0, # Maximum kl step between posterior estimations. Here it is ext. large to speed up computations.\n",
                "    silent=True, # Should there be print during each inner learning task\n",
                "    n_max_eval=200, # Maximum number of risk evaluations per task. This could be set even lower in this dummy setting.\n",
                "    n_estim_weights=10**3, # Number of samples generated to compute weights. Here it could be even lower.\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The Meta Learning algorithm can be called using the \"meta_learn\" method (using SGD by default, use meta_learn_batch for non standard GD).\n",
                "After each task has been calibrated once, the tasks inner learning hyperparameters can be updated (usually, no need to continue drawing a lot of parameters)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mlearn.meta_learn(epochs=1, eta=2/temperature, kl_max=1.0, mini_batch_size=25)\n",
                "mlearn.hyperparams.update({\"per_step\":20, \"chain_length\":1})\n",
                "mlearn.meta_learn(epochs=20, eta=1/temperature, kl_max=1.0, mini_batch_size=25)\n",
                "mlearn.hyperparams.update({\"n_estim_weights\":10**2}) # Gain more time\n",
                "mlearn.meta_learn(epochs=180, eta=0.5/temperature, kl_max=0.2, mini_batch_size=50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from surpbayes.meta_bayes.test_assess import eval_meta_hist\n",
                "res = eval_meta_hist(mlearn.hist_meta.meta_params()[::2], task_test, proba_map = proba_map, hyperparams = {\"per_step\": 50, \"chain_length\":1, \"silent\":True})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "low_quant, high_quant = np.apply_along_axis(lambda x: np.quantile(x, [0.2, 0.8]), 1, res).T\n",
                "test_perf = res.mean(1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "plt.fill_between(np.arange(len(low_quant)), low_quant, high_quant)\n",
                "plt.plot(test_perf, color=\"black\", linewidth=1)\n",
                "plt.xlabel(\"Meta training steps\")\n",
                "plt.ylabel(\"Generalisation bound\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The evolution of each independant test set can also be ascertained."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "for i in range(res.shape[1]):\n",
                "    plt.plot(res[:, i], linewidth=1.0)\n",
                "plt.xlabel(\"Meta training steps\")\n",
                "plt.ylabel(\"Generalisation bound\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Covariance case"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Choose dimension/Number of tasks\n",
                "d = 4\n",
                "true_dim = 1\n",
                "n_tasks_train = 50\n",
                "n_tasks_test = 20\n",
                "temperature = 0.1\n",
                "\n",
                "# Generate tasks\n",
                "def make_score(x):\n",
                "    def score(xs):\n",
                "        return ((x - xs) ** 2).sum(-1)\n",
                "\n",
                "    return score\n",
                "\n",
                "\n",
                "matrix = np.random.normal(0, 1, (true_dim, d))\n",
                "matrix = matrix / np.sum(matrix **2)\n",
                "x_middles = np.random.normal(0, 1.0, (n_tasks_train + n_tasks_test, true_dim)) @ matrix + np.random.normal(\n",
                "    0, 0.01, (n_tasks_train + n_tasks_test, d)\n",
                ")\n",
                "\n",
                "list_task = [\n",
                "    Task(make_score(x_mid), temperature=temperature, vectorized=True) for x_mid in x_middles\n",
                "]\n",
                "task_train = list_task[:n_tasks_train]\n",
                "task_test = list_task[n_tasks_train:]\n",
                "\n",
                "# Define distribution family\n",
                "proba_map = GaussianMap(d)\n",
                "\n",
                "# Define Meta Learning Environnement\n",
                "mlearn = MetaLearningEnv(\n",
                "    proba_map,\n",
                "    list_task=task_train,\n",
                "    per_step=25,\n",
                "    chain_length=1,\n",
                "    n_estim_weights=3 * proba_map.t_shape[0],\n",
                "    kl_max=1000.0,\n",
                "    silent=True,\n",
                "    n_max_eval=200,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Launch training (either through meta_learn or meta_learn_batch)\n",
                "mlearn.meta_learn_batch(epochs=5000, eta=0.2/temperature, kl_max=1.0, silent=True, kl_tol=10**-8)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(mlearn.hist_meta.meta_scores()[800:])\n",
                "plt.yscale(\"log\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "One can assess the value of the prior covariance eigenvalues, to see that the probability concentrate close to a 1D subspace."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mlearn.proba_map(mlearn.prior_param).vals"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Future improvements\n",
                "\n",
                "### Sample size for the inner task\n",
                "\n",
                "In the current implementation, the inner algorithm evaluates a fixed number of parameters generated from the current posterior. This might slow down the algorithm significantly, as once the space has been thoroughly explored, it is not necessary to evaluate many new points (at least not as much as during the early stages). The number of new points evaluated should be estimated depending on how well the current sample explores the posterior.\n",
                "\n",
                "On the same lines, the positions of the samples evaluated could be optimized.\n",
                "\n",
                "### Step size adaptation"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9.19 ('surpbayes-env')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.19"
        },
        "vscode": {
            "interpreter": {
                "hash": "d31d5f2aa31eecc22901d9ba58b3f69707441af03564d794e0b3013ba36a8800"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
