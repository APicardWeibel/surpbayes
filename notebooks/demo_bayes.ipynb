{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bayes module\n",
                "\n",
                "### Optimisation of Catoni's bound\n",
                "\n",
                "Given a score $S$, a parametric family of distributions $(\\nu_{\\theta})_{\\theta \\in \\Theta}$ and a prior distribution $\\pi$, we consider the minimisation problem\n",
                "\n",
                "$$\\hat{\\theta} = \\arg\\inf \\nu_{\\theta}[S] + \\lambda * KL(\\nu_{\\theta}, \\pi).$$\n",
                "\n",
                "The function pacbayes_minimize is designed to tackle such problems in the setting where $\\pi =  \\nu_{\\theta_0}$. This is in order to benefit from potential closed form expressions when computing the Kullback-Leibler divergence and its derivative.\n",
                "\n",
                "# SurPAC-CE algorithm\n",
                "\n",
                "SurPAC-CE (Surrogate PAC-Bayes for Catoni's bound minimisation on Exponential family) algorithm relies on surpbayes.proba submodule. The current form of the algorithm requires the family of distributions of interest to be an exponential family (classes \"surpbayes.proba.PreExpFamily\" and \"surpbayes.proba.ExponentialFamily\"). A slightly modified version is used in the case of Gaussian distributions (inherited from \"surpbayes.proba.PreExpFamily\").\n",
                "\n",
                "To demonstrate SurPAC-CE, a one dimensional problem is considered. However, SurPAC-CE can be used for problems of any dimension, although for large dimensions (> 100), some inner mechanisms (the weighing process) should be assessed and potentially modified. First, we define a tailor-made error function. For maximum efficiency, this function is vectorized.\n",
                "\n",
                "Note that for demonstration reasons, the hyperparameters of the algorithm are chosen as really conservative. This implies that the algorithm takes more time to converge than it could, and evaluates the score many more time than is really necessary. The resulting graphs are much smoother and the training mechanism better explained."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false
            },
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from surpbayes.bayes import pacbayes_minimize\n",
                "\n",
                "from surpbayes.proba import GaussianMap, TensorizedGaussianMap, BlockDiagGaussMap\n",
                "\n",
                "xs = np.linspace(-10, 10, 10**3)\n",
                "\n",
                "def fun(x):\n",
                "    return (np.tan(np.sqrt(1 + np.arctan(x - 1.57) ** 2) - 1.0) - 3) ** 2\n",
                "\n",
                "\n",
                "def gun(x):\n",
                "    return (np.tanh(x) + 1) / 2\n",
                "\n",
                "\n",
                "def S(x):\n",
                "    return (2 - fun(0.7 * (x - 0.4))) * gun(-0.2 - 0.7 * (x - 0.4)) + 3\n",
                "\n",
                "\n",
                "def score(x):\n",
                "    return (S(x[:, 0]+2) +.25) /3.25\n",
                "\n",
                "plt.plot(xs, score(xs[:, np.newaxis]), label=\"Score fun.\")\n",
                "plt.xlabel(\"param\")\n",
                "plt.ylabel(\"Score\")\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Calls to SurPAC-CE can be performed using the pacbayes_minimize function from bayes module as follow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from surpbayes.bayes import pacbayes_minimize\n",
                "from surpbayes.proba import GaussianMap\n",
                "\n",
                "from time import time\n",
                "\n",
                "gmap = GaussianMap(1)\n",
                "\n",
                "tic = time()\n",
                "opt_res_new = pacbayes_minimize(\n",
                "    score,\n",
                "    gmap,\n",
                "    # As not specified, prior used is the reference of gmap (i.e. standard Gaussian)\n",
                "    # As not specified, initial guess for posterior is the prior \n",
                "    optimizer=\"SurPAC-CE\",\n",
                "    temperature=0.015,  \n",
                "    # Hyperparameters\n",
                "    per_step=160, # score evaluations per step\n",
                "    chain_length=51,\n",
                "    n_estim_weights=10**4,\n",
                "    # regularisation parameters\n",
                "    dampen=0.7,\n",
                "    kl_max=0.1,\n",
                "    # dichotomy hyperparams for regularisation (can be omitted)\n",
                "    m_max=25,\n",
                "    xtol=10**-4,\n",
                "    kltol=10**-4,\n",
                "    # function call information\n",
                "    parallel=False,\n",
                "    vectorized=True,\n",
                "    # printing params\n",
                "    print_rec=5,\n",
                ")\n",
                "tac = time()\n",
                "print(\"Time elapsed:\", tac - tic)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The algorithm converged in less than 20 steps.\n",
                "The evolution of the approximation of the posterior can be easily represented in this 1D setting by plotting the densities."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false
            },
            "outputs": [],
            "source": [
                "xs = np.linspace(-5.0, 3.4, 2000)\n",
                "\n",
                "\n",
                "def repr_gauss(param):\n",
                "    distr = gmap(param)\n",
                "    return xs, distr.dens(xs[:, np.newaxis])\n",
                "\n",
                "\n",
                "plt.plot(*repr_gauss(opt_res_new.hist_param[0]), linewidth=1.0, label=\"prior\")\n",
                "for i, param in enumerate(opt_res_new.hist_param[:]):\n",
                "    if i % 1 == 0:\n",
                "\n",
                "        xs, ys = repr_gauss(param)\n",
                "        plt.plot(xs, ys, color=\"black\", linewidth=0.2)\n",
                "\n",
                "plt.plot(*repr_gauss(opt_res_new.hist_param[-1]), linewidth=1.0, label=\"posterior\")\n",
                "plt.title(\"Evolution of the posterior estimation\")\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let us see what happens in detail during this training phase."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from surpbayes.bayes.surpac.weighing import get_weights_mc\n",
                "from surpbayes.bayes.surpac.surpac_solver import exp_approximation\n",
                "from scipy.stats import norm\n",
                "\n",
                "np.random.seed(0)\n",
                "\n",
                "temp = 0.02\n",
                "dampen = 0.975\n",
                "n_sample = 40\n",
                "\n",
                "xs = np.linspace(-3.5, 3.5, 1000)\n",
                "\n",
                "prior_param = gmap.ref_param\n",
                "prior = gmap(prior_param)\n",
                "prior_norm = norm(loc=prior.means[0], scale=np.sqrt(prior.cov[0, 0]))\n",
                "\n",
                "sample = prior(n_sample)\n",
                "score_sample = score(sample)\n",
                "\n",
                "# Plotting\n",
                "fig, ax1 = plt.subplots()\n",
                "ax2 = ax1.twinx()\n",
                "\n",
                "ax2.set_ylabel(\"Density\")\n",
                "ax1.set_ylabel(\"Score\")\n",
                "\n",
                "ax2.fill_between(xs, prior_norm.pdf(xs), color=\"0.7\", alpha=0.2, label=\"prior\")\n",
                "ax1.plot(\n",
                "    xs, score(xs[:, np.newaxis]), \"--\", linewidth=1.0, color=\"black\", label=\"Score\"\n",
                ")\n",
                "\n",
                "ax1.plot(sample, score_sample, \"o\", markersize=6.0, label=\"Score eval.\")\n",
                "ax1.set_ylim(-0.3, 3.2)\n",
                "ax1.legend(loc=1)\n",
                "ax2.legend(loc=2)\n",
                "\n",
                "fig.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Parameters are drawn from the current posterior approximation. The scores of these parameters are evaluated and added to the stack of parameters evaluated so far."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Infer estimation of score\n",
                "weights = get_weights_mc(prior, sample, n_sample_estim=10**5)\n",
                "m_score = np.sum(score_sample * weights)\n",
                "\n",
                "# For plottinf\n",
                "fig, ax1 = plt.subplots()\n",
                "ax2 = ax1.twinx()\n",
                "\n",
                "ax2.set_ylabel(\"Density\")\n",
                "# ax1.set_ylabel(\"Score\")\n",
                "\n",
                "\n",
                "ax2.fill_between(xs, prior_norm.pdf(xs), color=\"0.7\", alpha=0.2, label=\"prior\")\n",
                "\n",
                "\n",
                "loc_sample = sample.copy()\n",
                "loc_weights = get_weights_mc(prior, loc_sample, n_sample_estim=10**6)\n",
                "\n",
                "sorter = np.argsort(loc_sample.flatten())\n",
                "loc_sample = loc_sample[sorter]\n",
                "loc_weights = loc_weights[sorter]\n",
                "\n",
                "cut = (loc_sample[1:] + loc_sample[:-1]) / 2\n",
                "\n",
                "loc_weights_renorm = loc_weights.copy()\n",
                "loc_weights_renorm[0] = 0.0\n",
                "loc_weights_renorm[-1] = 0.0\n",
                "\n",
                "loc_weights_renorm[1:-1] = loc_weights_renorm[1:-1] / (cut[1:, 0] - cut[:-1, 0])\n",
                "\n",
                "cut_plot = [xs[0]] + list(np.array([[a, a] for a in cut]).flatten()) + [xs[-1]]\n",
                "weights_for_plot = list(np.array([[a, a] for a in loc_weights_renorm]).flatten())\n",
                "ax2.plot(cut_plot, weights_for_plot, color=\"black\", label=\"Weights\", linewidth=1)\n",
                "\n",
                "ax2.plot(loc_sample, loc_weights_renorm, \"o\", color=\"tab:blue\", markersize=4)\n",
                "ax1.set_yticks([])\n",
                "fig.tight_layout()\n",
                "\n",
                "ax2.legend(loc=2)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Weights are computed for each parameter in the stack of evaluated parameter."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "T_s = gmap.T(sample)\n",
                "T_approx = exp_approximation(Ts=T_s, scores=score_sample, weights=weights)\n",
                "\n",
                "T_xs = gmap.T(xs[:, np.newaxis])\n",
                "score_approx = (T_xs * T_approx).sum(-1)\n",
                "\n",
                "fig, ax1 = plt.subplots()\n",
                "ax2 = ax1.twinx()\n",
                "\n",
                "ax2.set_ylabel(\"Density\")\n",
                "ax1.set_ylabel(\"Score\")\n",
                "\n",
                "ax2.fill_between(xs, prior_norm.pdf(xs), color=\"0.7\", alpha=0.2, label=\"prior\")\n",
                "ax1.plot(\n",
                "    xs, score(xs[:, np.newaxis]), \"--\", linewidth=1.0, color=\"black\", label=\"Score\"\n",
                ")\n",
                "\n",
                "ax1.plot(sample, score_sample, \"o\", markersize=9.0, label=\"Score eval.\")\n",
                "ax1.set_ylim(-0.3, 3.2)\n",
                "\n",
                "score_sample_approx = (T_s * T_approx).sum(-1)\n",
                "\n",
                "delta = np.sum(weights * (score_sample - score_sample_approx))\n",
                "\n",
                "ax1.plot(xs, delta + score_approx, color=\"peru\", linewidth=2.5, label=\"Score approx.\")\n",
                "fig.tight_layout()\n",
                "ax1.legend(loc=1)\n",
                "ax2.legend(loc=2)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The weighing process is used to compute the best L2 approximation of the score of a given form (here for gaussians, quadratic forms)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "delta = np.sum(weights * (score_sample - (T_s * T_approx).sum(-1)))\n",
                "\n",
                "T_prior = gmap.param_to_T(prior_param)\n",
                "T_updt_dir = -(temp**-1) * T_approx\n",
                "T_new = T_prior + (1 - dampen) * T_updt_dir\n",
                "\n",
                "post_param = gmap.T_to_param(T_new)\n",
                "\n",
                "post = gmap(post_param)\n",
                "post_norm = norm(loc=post.means[0], scale=np.sqrt(post.cov[0, 0]))\n",
                "\n",
                "fig, ax1 = plt.subplots()\n",
                "ax2 = ax1.twinx()\n",
                "\n",
                "ax2.set_ylabel(\"Density\")\n",
                "ax1.set_ylabel(\"Score\")\n",
                "\n",
                "ax2.fill_between(xs, prior_norm.pdf(xs), color=\"0.7\", alpha=0.2, label=\"prior\")\n",
                "\n",
                "ax1.set_ylim(-0.3, 3.2)\n",
                "\n",
                "fig.tight_layout()\n",
                "\n",
                "ax1.plot(xs, delta + score_approx, color=\"peru\", linewidth=2.5, label=\"Score approx.\")\n",
                "fig.tight_layout()\n",
                "\n",
                "ax2.fill_between(\n",
                "    xs, post_norm.pdf(xs), color=\"skyblue\", alpha=0.2, label=\"post. approx.\"\n",
                ")\n",
                "ax2.set_ylabel(\"Density\")\n",
                "\n",
                "ax1.set_ylabel(\"Score\")\n",
                "\n",
                "fig.tight_layout()\n",
                "ax1.legend(loc=1)\n",
                "ax2.legend(loc=2)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The approximated score is used to compute the posterior update.\n",
                "This learning cycle is then repeated until either convergence is achieved or the maximal number of optimisation steps is achieved.\n",
                "\n",
                "The algorithm is now showcased on a more complex function, two dimensional for plotting purposes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from surpbayes.bayes import pacbayes_minimize\n",
                "from surpbayes.proba import GaussianMap, TensorizedGaussianMap\n",
                "\n",
                "# For plotting purposes\n",
                "from math import pi\n",
                "\n",
                "angles = np.linspace(0, 2.001 * pi, 1000)\n",
                "circle = np.array([np.cos(angles), np.sin(angles)])\n",
                "\n",
                "\n",
                "def half_cov(cov):\n",
                "    vals, vects = np.linalg.eigh(cov)\n",
                "    return (np.sqrt(vals) * vects) @ vects.T\n",
                "\n",
                "\n",
                "def repr_gauss(mean, cov, rad=1.0):\n",
                "    \"\"\"Visual representation of a 2D Gaussian distribution through ellipsis\n",
                "    (representing highest density region, e.g. smallest volume region of a\n",
                "    given probability (depending on rad))\"\"\"\n",
                "    loc_circle = circle.copy()\n",
                "    return mean + rad * (half_cov(cov) @ loc_circle).T\n",
                "\n",
                "\n",
                "arr_1 = np.array([0.0, 1.0])\n",
                "arr_2 = np.array([1, -1])\n",
                "_shift = np.array([0.0, 0.5])\n",
                "\n",
                "\n",
                "def score(x):\n",
                "    z = x - _shift\n",
                "    return 4 * np.arctan(0.5 * ((z @ arr_1 - 1.0) ** 2 + 100.0 * (z @ arr_2) ** 2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gmap = GaussianMap(2)\n",
                "\n",
                "tic = time()\n",
                "opt_res_new = pacbayes_minimize(\n",
                "    score,\n",
                "    gmap,\n",
                "    temperature=0.1,  \n",
                "    optimizer=\"score_approx\",\n",
                "    per_step=96,\n",
                "    parallel=False,\n",
                "    vectorized=True,\n",
                "    print_rec=10,\n",
                "    chain_length=141,\n",
                "    n_estim_weights=2 * 10**4,\n",
                "    kl_max=0.04,\n",
                "    m_max=20,\n",
                "    xtol=10**-7,\n",
                "    kltol=10**-7,\n",
                "    silent=False,\n",
                "    dampen=0.1,\n",
                ")\n",
                "tac = time()\n",
                "print(\"Time elapsed:\", tac - tic)\n",
                "\n",
                "for i, param in enumerate(opt_res_new.hist_param[:]):\n",
                "    if i % 1 == 0:\n",
                "        proba = gmap(param)\n",
                "        proba_repr = repr_gauss(proba.means, proba.cov)\n",
                "        plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)\n",
                "        plt.plot(proba.means[0], proba.means[1], \"x\", color = \"blue\")\n",
                "\n",
                "plt.title(\"Evolution of the posterior estimation\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We plot the evolutions of the posterior in conjonction with the score heatmap"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import seaborn as sns\n",
                "\n",
                "x_min = -1.1\n",
                "x_max = 1.8\n",
                "n_x = 601\n",
                "\n",
                "y_min = -1.1\n",
                "y_max = 1.8\n",
                "n_y = 601\n",
                "\n",
                "x_axis_labels = np.linspace(\n",
                "    x_min, x_max, n_x\n",
                ")  # Avoid renormalisation issue at the angles\n",
                "y_axis_labels = np.linspace(y_min, y_max, n_y)\n",
                "\n",
                "\n",
                "def shift(xs, ys):\n",
                "    return n_x * (xs - x_min) / (x_max - x_min), n_y * (ys - y_min) / (y_max - y_min)\n",
                "\n",
                "\n",
                "values = np.array(np.meshgrid(y_axis_labels, x_axis_labels)).T\n",
                "\n",
                "z = score(values)\n",
                "alpha = np.linspace(0, 8, 100)\n",
                "\n",
                "sns.heatmap(\n",
                "    z.T,\n",
                "    xticklabels=y_axis_labels,\n",
                "    yticklabels=x_axis_labels,\n",
                "    cmap=sns.color_palette(\"Blues\", as_cmap=True),\n",
                ")\n",
                "# sns.heatmap(z)\n",
                "for i, param in enumerate(opt_res_new.hist_param):\n",
                "    if i % 2 == 0:\n",
                "        proba = gmap(param)\n",
                "        proba_repr = repr_gauss(proba.means, proba.cov)\n",
                "        #         proba_repr  = repr_gauss(np.array([1.38,-1.1]), 0.01 * np.eye(2))\n",
                "\n",
                "        xs, ys = shift(proba_repr[:, 0], proba_repr[:, 1])\n",
                "        sns.lineplot(x=xs, y=ys, sort=False, color=\"yellow\", linewidth=1.0)\n",
                "\n",
                "\n",
                "proba = gmap(opt_res_new.opti_param)\n",
                "proba_repr = repr_gauss(proba.means, proba.cov)\n",
                "\n",
                "xs, ys = shift(proba_repr[:, 0], proba_repr[:, 1])\n",
                "sns.lineplot(x=xs, y=ys, sort=False, color=\"red\", linewidth=1.0)\n",
                "\n",
                "plt.xticks([])\n",
                "plt.yticks([])\n",
                "# Note that the plot is turned compared to the previous version, due to seaborn's behavior for heatmap...\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The procedure can also be investigated for other family of distributions, for instance Gaussians with fixed variance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from surpbayes.proba import FactCovGaussianMap, FixedCovGaussianMap\n",
                "from time import time\n",
                "\n",
                "cov = np.array([[0.5, 0.4], [0.4, 0.5]])\n",
                "\n",
                "ficgm = FixedCovGaussianMap(2, cov=cov) # Variance is fixed\n",
                "facgm = FactCovGaussianMap(2, cov=cov) # Variance is fixed up to a multiplicative factor\n",
                "\n",
                "proba_map = facgm\n",
                "tic = time()\n",
                "opt_res_new = pacbayes_minimize(\n",
                "    score,\n",
                "    proba_map,\n",
                "    temperature=0.01,  \n",
                "    optimizer=\"score_approx\",\n",
                "    per_step=160,\n",
                "    parallel=False,\n",
                "    vectorized=True,\n",
                "    print_rec=10,\n",
                "    chain_length=61,\n",
                "    n_estim_weights=2*10**4,\n",
                "    kl_max=0.05,\n",
                "    m_max=20,\n",
                "    xtol=10**-9,\n",
                "    #     alpha_filter=0.99,\n",
                "    silent=False,\n",
                "    dampen=0.5,\n",
                ")\n",
                "tac = time()\n",
                "print(tac - tic)\n",
                "for i, param in enumerate(opt_res_new.hist_param):\n",
                "    if i % 1 == 0:\n",
                "        proba = proba_map(param)\n",
                "        proba_repr = repr_gauss(proba.means, proba.cov)\n",
                "        plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)\n",
                "        plt.plot(proba.means[0], proba.means[1], \"x\")\n",
                "\n",
                "plt.title(\"Evolution of the posterior estimation\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "An interesting consideration is how much the posterior distribution gives weights to draws from previous distributions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false
            },
            "outputs": [],
            "source": [
                "from surpbayes.bayes.plot.surpac import plot_weights_per_gen\n",
                "\n",
                "plot_weights_per_gen(opt_res_new, n_sample_estim_weight=10**6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The evolution of the pushforward of the score by the posterior distribution can also be evaluated using the following plot (this is quite long to generate, since the weighing process is repeated for each previous parameter)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from surpbayes.bayes.plot.optim_result import plot_score_evol\n",
                "\n",
                "plot = plot_score_evol(opt_res_new, n_sample_weight_estim=10**4)\n",
                "plot.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A simpler version of this plot can be obtained by using \"plot_scores\" function as follows:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from surpbayes.bayes.plot import plot_hist_bayes, plot_scores\n",
                "\n",
                "plot = plot_scores(sample_val=opt_res_new.sample_val, marker=\"x\", s=0.03)\n",
                "plot.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finally, the evolution of the density of the pushforward of the score can be plotted using \"plot_score_push_begin_end\", which relies on a weighted kde estimator"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from surpbayes.bayes.plot import plot_score_push_begin_end\n",
                "\n",
                "plot = plot_score_push_begin_end(opt_res_new)\n",
                "plot.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Gradient based algorithm\n",
                "\n",
                "The gradient based routine can be called by setting the parameter 'pac_bayes_solver' to \"corr_weights\" or \"knn\". It is advised to use 'corr_weights'.\n",
                "It is normal behavior that the optimisation procedure raises some ProbaBadGrad warnings. These indicate that a problematic gradient estimation was rejected as it damaged significantly the score. No need to worry about those."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gauss_map = GaussianMap(2)\n",
                "\n",
                "# We define the prior as the reference gaussian distribution, i.e. N(0,Id)\n",
                "prior_param = gauss_map.ref_param\n",
                "\n",
                "# To minimize PAC-Bayes bound, we use the pacbayes_minimize function.\n",
                "opt_res_grad = pacbayes_minimize(\n",
                "    score,\n",
                "    gauss_map,\n",
                "    prior_param=prior_param,\n",
                "    temperature=0.1,  \n",
                "    per_step=160,\n",
                "    optimizer=\"corr_weights\",\n",
                "    gen_decay=np.log(1.2),\n",
                "    k=160 * 20,\n",
                "    parallel=False,\n",
                "    vectorized=True,\n",
                "    print_rec=50,\n",
                "    chain_length=501,\n",
                "    refuse_conf=0.9,\n",
                "    momentum=0.9,\n",
                "    eta=0.05,\n",
                "    silent=False,\n",
                ")\n",
                "\n",
                "# It is normal behavior that the optimisation procedure raises some ProbaBadGrad warnings.\n",
                "# These indicate that a problematic gradient estimation was rejected as it damaged significantly\n",
                "# the score. No need to worry about those.\n",
                "\n",
                "# We can access the parameter describing the posterior through the opti_param attribute\n",
                "post_param = opt_res_grad.opti_param"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "proba = gmap(opt_res_grad.hist_param[0])\n",
                "proba_repr = repr_gauss(proba.means, proba.cov)\n",
                "plt.plot(proba_repr[:, 0], proba_repr[:, 1], linewidth=1.0, label=\"prior\")\n",
                "\n",
                "for i, param in enumerate(opt_res_grad.hist_param[:]):\n",
                "    if i % 3 == 0:\n",
                "        proba = gmap(param)\n",
                "        proba_repr = repr_gauss(proba.means, proba.cov)\n",
                "        plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)\n",
                "#         plt.plot(proba.means[0], proba.means[1], \"x\")\n",
                "\n",
                "proba = gmap(opt_res_grad.opti_param)\n",
                "proba_repr = repr_gauss(proba.means, proba.cov)\n",
                "plt.plot(\n",
                "    proba_repr[:, 0],\n",
                "    proba_repr[:, 1],\n",
                "    linewidth=1.2,\n",
                "    color=\"crimson\",\n",
                "    label=\"posterior\",\n",
                ")\n",
                "plt.plot([1], [1.5], \"x\", c=\"0.0\", markersize=10, label=\"Glob. Min.\")\n",
                "\n",
                "plt.title(\"Evolution of the posterior estimation (corr weights)\")\n",
                "plt.legend()\n",
                "plt.xticks([-1.0, 0.0, 1.0])\n",
                "plt.yticks([-1.0, 0.0, 1.0])\n",
                "plt.savefig(\"corr_weights_training.png\")\n",
                "plt.show()\n",
                "\n",
                "\n",
                "# for i, param in enumerate(opt_res_grad.hist_param[:-450]):\n",
                "#     if i % 2 == 0:\n",
                "#         proba = gauss_map(param)\n",
                "#         proba_repr = repr_gauss(proba.means, proba.cov)\n",
                "#         plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# The distribution then tries to shift towards the correct mean value\n",
                "for i, param in enumerate(opt_res_grad.hist_param[50:500:30]):\n",
                "    if i % 1 == 0:\n",
                "        proba = gauss_map(param)\n",
                "        proba_repr = repr_gauss(proba.means, proba.cov)\n",
                "        plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# The evolution of the PAC-Bayes objective can also be tracked:\n",
                "plt.plot(opt_res_grad.hist_score)\n",
                "plt.yscale(\"log\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_score_push_begin_end(opt_res_grad)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Uniform priors - Gaussian computations\n",
                "\n",
                "The proba module offers a class of distributions on the hypercube benefitting from Gaussian like interpretation when the distribution are sufficiently concentrated and exact computations for KL."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from surpbayes.proba.gauss import GaussHypercubeMap\n",
                "\n",
                "dim = 2\n",
                "\n",
                "# Toy score function\n",
                "def score(x):\n",
                "    return np.arctan(\n",
                "        0.2 * (x @ np.array([1.0, 0.0], dtype=np.float64) - 0.6) ** 2\n",
                "        + 20 * (x @ np.array([1.0, -2.0], dtype=np.float64)) ** 2\n",
                "    )\n",
                "\n",
                "\n",
                "pmap = GaussHypercubeMap(2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Since this family of probability is of form $f(X)$ for $X$ spanning Gaussian, this defines an exponential family, and hence Catoni's bound can be minimized using SurPAC."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "opt_res = pacbayes_minimize(\n",
                "    score,\n",
                "    pmap,\n",
                "    temperature=0.01,  \n",
                "    per_step=80,\n",
                "    dampen=0.1,\n",
                "    kl_max=0.2,\n",
                "    parallel=False,\n",
                "    vectorized=True,\n",
                "    print_rec=10,\n",
                "    chain_length=51,\n",
                "    silent=False,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The posterior can adapt to scores with strong identifiability issues such as Rosenbrock, since the probabilities can exhibit strong correlation structure"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import seaborn as sns\n",
                "\n",
                "proba = pmap(opt_res.opti_param)\n",
                "# The log density of the function can be accessed through log_dens\n",
                "x_axis_labels = np.linspace(10**-4, 1 - 10**-4, 121)\n",
                "y_axis_labels = np.linspace(10**-4, 1 - 10**-4, 121)\n",
                "\n",
                "values = np.array(np.meshgrid(y_axis_labels, x_axis_labels)).T\n",
                "z = proba.dens(values)\n",
                "\n",
                "sns.heatmap(z, xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
                "plt.title(\"Posterior distribution\")\n",
                "plt.xticks([])\n",
                "plt.yticks([])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(opt_res.hist_score, label=\"Evolution of PAC-Bayes obj.\")\n",
                "plt.yscale(\"log\")\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Tests for problems of larger dimensions\n",
                "\n",
                "d = 80\n",
                "k = 5\n",
                "\n",
                "blocks = [list(range(i * k, min(d, (i + 1) * k))) for i in range(int(np.ceil(d / k)))]\n",
                "\n",
                "mat = (np.random.uniform(0.5, 2, d)) * np.random.normal(0, 1, (4 * d, d))\n",
                "mat = mat.T @ mat / (4 * d)\n",
                "\n",
                "print(np.linalg.eigvalsh(mat))\n",
                "\n",
                "center = np.random.normal(0, 1, d)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def score(xs):\n",
                "    deltas = np.arctan(xs - center)\n",
                "    return (deltas * (deltas @ mat)).sum(-1)\n",
                "\n",
                "\n",
                "bgmap = BlockDiagGaussMap(blocks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "opt_res = pacbayes_minimize(\n",
                "    score,\n",
                "    bgmap,\n",
                "    per_step=800,\n",
                "    kl_max=3.0,\n",
                "    temperature=0.1,\n",
                "    chain_length=20,\n",
                "    vectorized=True,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(opt_res.hist_score)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Checking stability"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def score(x):\n",
                "    return -(\n",
                "        1.2 * np.exp(-4 * (x @ [1] - 4) ** 2) + 1.0 * np.exp(-1.0 * (x @ [1]) ** 2)\n",
                "    )\n",
                "\n",
                "\n",
                "xs = np.linspace(-4, 8, 4000).reshape((4000, 1))\n",
                "\n",
                "plt.plot(xs, score(xs))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from surpbayes.accu_xy import AccuSampleVal\n",
                "\n",
                "xs = np.linspace(3.5, 4.5, 1000).reshape((1000, 1))\n",
                "accu = AccuSampleVal((1,), 1000)\n",
                "accu.add(xs, score(xs))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "2 * (1 - norm.cdf(4))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy.stats import norm\n",
                "\n",
                "gmap = GaussianMap(1)\n",
                "\n",
                "tic = time()\n",
                "opt_res_stable = pacbayes_minimize(\n",
                "    score,\n",
                "    gmap,\n",
                "    #     facgm,\n",
                "    #     prev_eval=accu, # Check if this improves matter or not\n",
                "    temperature=0.002,  \n",
                "    pac_bayes_solver=\"score_approx\",\n",
                "    prior_param=np.array([[0.0], [2.0]]),\n",
                "    post_param=np.array([[4.0], [1.0]]),\n",
                "    per_step=100,\n",
                "    parallel=True,\n",
                "    vectorized=False,\n",
                "    print_rec=10,\n",
                "    chain_length=101,\n",
                "    n_estim_weights=10**6,\n",
                "    kl_max=0.1,\n",
                "    m_max=20,\n",
                "    xtol=10**-8,\n",
                "    kltol=10**-8,\n",
                "    alpha_filter=1.0,\n",
                "    silent=False,\n",
                "    dampen=0.01,\n",
                ")\n",
                "tac = time()\n",
                "print(\"Time elapsed:\", tac - tic)\n",
                "\n",
                "xs = np.linspace(-4, 6, 1000)\n",
                "\n",
                "for i, param in enumerate(opt_res_stable.hist_param[:]):\n",
                "    if i % 2 == 0:\n",
                "        proba = gmap(param)\n",
                "        plt.plot(\n",
                "            xs,\n",
                "            norm(proba.means[0], np.sqrt(proba.cov[0])).pdf(xs),\n",
                "            color=\"black\",\n",
                "            linewidth=0.4,\n",
                "        )\n",
                "#         proba_repr = repr_gauss(proba.means, proba.cov)\n",
                "#         plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)\n",
                "#         plt.plot(proba.means[0], proba.means[1], \"x\")\n",
                "\n",
                "plt.title(\"Evolution of the posterior estimation\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "opt_res_stable.opti_param"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for i, param in enumerate(opt_res_stable.hist_param[10:30]):\n",
                "    if i % 1 == 0:\n",
                "        proba = gmap(param)\n",
                "        plt.plot(\n",
                "            xs,\n",
                "            norm(proba.means[0], np.sqrt(proba.cov[0])).pdf(xs),\n",
                "            color=\"black\",\n",
                "            linewidth=0.4,\n",
                "        )\n",
                "#         proba_repr = repr_gauss(proba.means, proba.cov)\n",
                "#         plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)\n",
                "#         plt.plot(proba.means[0], proba.means[1], \"x\")\n",
                "\n",
                "plt.title(\"Evolution of the posterior estimation\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9.19 ('surpbayes-env')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.19"
        },
        "vscode": {
            "interpreter": {
                "hash": "d31d5f2aa31eecc22901d9ba58b3f69707441af03564d794e0b3013ba36a8800"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
