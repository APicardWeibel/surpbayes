{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes module\n",
    "\n",
    "### Variational inference\n",
    "\n",
    "Given a score $S$, a parametric family of distributions $(\\nu_{\\theta})_{\\theta \\in \\Theta}$ and a prior distribution $\\pi$, we consider the variational problem\n",
    "\n",
    "$$\\hat{\\theta} = \\arg\\inf \\nu_{\\theta}[S] + \\lambda * KL(\\nu_{\\theta}, \\pi).$$\n",
    "\n",
    "The function variational_inference is designed to tackle such problems in the setting where $\\pi =  \\nu_{\\theta_0}$. This is in order to benefit from potential closed form expressions when computing the Kullback--Leibler divergence and its derivative.\n",
    "\n",
    "# VarBUQ algorithm\n",
    "\n",
    "VarBUQ algorithm relies on surpbayes.proba submodule. The current form of the algorithm requires the family of distributions of interest to be an exponential family (classes \"surpbayes.proba.PreExpFamily\" and \"surpbayes.proba.ExponentialFamily\"). A slightly modified version is used in the case of Gaussian distributions (inherited from \"surpbayes.proba.PreExpFamily\").\n",
    "\n",
    "To demonstrate VarBUQ, a one dimensional problem is considered. However, VarBUQ can be used for problems of any dimension. First, we define a tailor-made error function. For maximum efficiency, this function is vectorized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from surpbayes.bayes import iter_prior, variational_inference, iter_prior_vi\n",
    "\n",
    "from surpbayes.proba import GaussianMap, TensorizedGaussianMap, BlockDiagGaussMap\n",
    "\n",
    "xs = np.linspace(-10, 10, 10**3)\n",
    "\n",
    "\n",
    "def fun(x):\n",
    "    return (np.tan(np.sqrt(1 + np.arctan(x - 1.57) ** 2) - 1.0) - 3) ** 2\n",
    "\n",
    "\n",
    "def gun(x):\n",
    "    return (np.tanh(x) + 1) / 2\n",
    "\n",
    "\n",
    "def S(x):\n",
    "    return (2 - fun(0.7 * (x - 0.4))) * gun(-0.2 - 0.7 * (x - 0.4)) + 3\n",
    "\n",
    "\n",
    "def score(x):\n",
    "    return (S(x[:, 0]+3) +.25) /3.25\n",
    "\n",
    "\n",
    "plt.plot(xs, score(xs[:, np.newaxis]), label=\"Score fun.\")\n",
    "plt.xlabel(\"param\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = score(xs[:, np.newaxis])\n",
    "\n",
    "np.min(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from surpbayes.bayes import variational_inference, OptimResultVI\n",
    "from surpbayes.proba import GaussianMap\n",
    "from time import time\n",
    "\n",
    "gmap = GaussianMap(1)\n",
    "\n",
    "tic = time()\n",
    "opt_res_news = [\n",
    "    variational_inference(\n",
    "    score,\n",
    "    gmap,\n",
    "    optimizer=\"score_approx\",\n",
    "    temperature=0.015,  # the lambda term in the variational inference problem\n",
    "    per_step=4,\n",
    "    parallel=False,\n",
    "    vectorized=True,\n",
    "    print_rec=2,\n",
    "    chain_length=101,\n",
    "    n_estim_weights=4 * 10**4,\n",
    "    kl_max=0.25,\n",
    "    m_max=15,\n",
    "    xtol=10**-12,\n",
    "    kltol=10**-12,\n",
    "    dampen=0.0,\n",
    "    silent=True)\n",
    " for _ in range(40)]\n",
    "tac = time()\n",
    "print(\"Time elapsed:\", tac - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_per_step = 4\n",
    "\n",
    "opt_res_grads = [\n",
    "    variational_inference(\n",
    "        score,\n",
    "        gmap,\n",
    "        prior_param=gmap.ref_param,\n",
    "        temperature=0.015,  # the lambda term in the variational inference problem\n",
    "        per_step=grad_per_step,\n",
    "        optimizer=\"corr_weights\",\n",
    "        gen_decay=100,\n",
    "        k=grad_per_step,\n",
    "        parallel=False,\n",
    "        vectorized=True,\n",
    "        print_rec=100,\n",
    "        chain_length=101,\n",
    "        refuse_conf=1.1,\n",
    "        momentum=0.9,\n",
    "        eta=2.0,\n",
    "        silent=True,\n",
    "    ) \n",
    "for _ in range(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mscore(fun, prob, n= 10**6):\n",
    "    samples = prob(n)\n",
    "    return fun(samples).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_plot(fun, pmap, opt_res:OptimResultVI, per_step:int, temp:float, p_param = None, n= 10**4):\n",
    "    if p_param is None:\n",
    "        p_param = pmap.ref_param\n",
    "    n_steps = len(opt_res.hist_param)\n",
    "    vscores = np.zeros(n_steps)\n",
    "    for i, param in enumerate(opt_res.hist_param):\n",
    "        mscore = clean_mscore(fun, pmap(param), n)\n",
    "        vscores[i] = mscore + temp * pmap.kl(param, p_param)\n",
    "    return np.arange(n_steps) * per_step, vscores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_grad = [\n",
    "    prep_plot(score, gmap, opt_res_grad, per_step=grad_per_step, temp=0.015)\n",
    "    for opt_res_grad in opt_res_grads]\n",
    "\n",
    "xs_grad = data_grad[0][0]\n",
    "ys_grad = np.array([d[1] for d in data_grad])\n",
    "\n",
    "min_grad, max_grad = np.apply_along_axis(lambda x : np.quantile(x, [0.2, 0.8]), 0,  ys_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = [\n",
    "    prep_plot(score, gmap, opt_res_new, per_step=4, temp=0.015)\n",
    "    for opt_res_new in opt_res_news]\n",
    "\n",
    "xs_new = data_new[0][0]\n",
    "ys_new = np.array([d[1] for d in data_new])\n",
    "\n",
    "min_new, max_new = np.apply_along_axis(lambda x : np.quantile(x, [0.2, 0.8]), 0,  ys_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_grad, max_grad = np.apply_along_axis(lambda x : np.quantile(x, [0.1, 0.9]), 0,  ys_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ys in ys_grad:\n",
    "    plt.plot(xs_grad, ys, linewidth=0.2, color=\"tab:blue\")\n",
    "# plt.fill_between(xs_grad, min_grad, max_grad)\n",
    "for ys in ys_new:\n",
    "# plt.fill_between(xs_new, min_new, max_new)\n",
    "    plt.plot(xs_new, ys, linewidth=0.2, color=\"tab:orange\")\n",
    "\n",
    "plt.xlabel(\"Number of risk evaluations\")\n",
    "plt.ylabel(\"Optimisation objective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for opt_res_new in opt_res_news:\n",
    "    plt.plot(*prep_plot(score, gmap, opt_res_new, per_step=4, temp=0.015), color=\"tab:blue\", linewidth=0.5)\n",
    "for opt_res_grad in opt_res_grads:\n",
    "    plt.plot(*prep_plot(score, gmap, opt_res_grad, per_step=4, temp=0.015), color=\"tab:orange\", linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = len(opt_res_new.hist_score)\n",
    "per_step = 4\n",
    "\n",
    "plt.plot(per_step * np.arange(n_steps), opt_res_new.hist_score)\n",
    "\n",
    "n_steps = len(opt_res_grad.hist_score)\n",
    "plt.plot(32 * np.arange(n_steps), opt_res_grad.hist_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surpbayes.bayes import variational_inference\n",
    "from surpbayes.proba import GaussianMap\n",
    "\n",
    "from time import time\n",
    "\n",
    "gmap = GaussianMap(1)\n",
    "\n",
    "tic = time()\n",
    "opt_res_new = variational_inference(\n",
    "    score,\n",
    "    gmap,\n",
    "    optimizer=\"score_approx\",\n",
    "    temperature=0.015,  # the lambda term in the variational inference problem\n",
    "    per_step=160,\n",
    "    parallel=False,\n",
    "    vectorized=True,\n",
    "    print_rec=2,\n",
    "    chain_length=51,\n",
    "    n_estim_weights=10**5,\n",
    "    kl_max=0.1,\n",
    "    m_max=20,\n",
    "    xtol=10**-6,\n",
    "    kltol=10**-6,\n",
    "    dampen=0.7,\n",
    ")\n",
    "tac = time()\n",
    "print(\"Time elapsed:\", tac - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_res_grad = variational_inference(\n",
    "    score,\n",
    "    gmap,\n",
    "    temperature=0.015,  \n",
    "    per_step=8,\n",
    "    optimizer=\"corr_weights\",\n",
    "    # gen_decay=np.log(1.2),\n",
    "    # k=8 * 20,\n",
    "    parallel=False,\n",
    "    vectorized=True,\n",
    "    print_rec=2,\n",
    "    chain_length=101,\n",
    "    refuse_conf=1.0,\n",
    "    # momentum=0.9,\n",
    "    eta=1.0,\n",
    "    silent=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm converged in less than 20 steps.\n",
    "The evolution of the approximation of the posterior can be easily represented in this 1D setting by plotting the densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xs = np.linspace(-3.5, 3.9, 2000)\n",
    "\n",
    "\n",
    "def repr_gauss(param):\n",
    "    distr = gmap(param)\n",
    "    return xs, distr.dens(xs[:, np.newaxis])\n",
    "\n",
    "\n",
    "plt.plot(*repr_gauss(opt_res_new.hist_param[0]), linewidth=1.0, label=\"prior\")\n",
    "for i, param in enumerate(opt_res_new.hist_param[:]):\n",
    "    if i % 1 == 0:\n",
    "\n",
    "        xs, ys = repr_gauss(param)\n",
    "        plt.plot(xs, ys, color=\"black\", linewidth=0.2)\n",
    "\n",
    "plt.plot(*repr_gauss(opt_res_new.hist_param[-1]), linewidth=1.0, label=\"posterior\")\n",
    "plt.title(\"Evolution of the posterior estimation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what happens in detail during this training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surpbayes.bayes.score_approx.weighing import get_weights_mc\n",
    "from surpbayes.bayes.score_approx.score_approx_solver import exp_approximation\n",
    "from scipy.stats import norm\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "temp = 0.02\n",
    "dampen = 0.975\n",
    "n_sample = 40\n",
    "\n",
    "xs = np.linspace(-3.5, 3.5, 1000)\n",
    "\n",
    "prior_param = gmap.ref_param\n",
    "prior = gmap(prior_param)\n",
    "prior_norm = norm(loc=prior.means[0], scale=np.sqrt(prior.cov[0, 0]))\n",
    "\n",
    "sample = prior(n_sample)\n",
    "score_sample = score(sample)\n",
    "\n",
    "# Plotting\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.set_ylabel(\"Density\")\n",
    "ax1.set_ylabel(\"Score\")\n",
    "\n",
    "ax2.fill_between(xs, prior_norm.pdf(xs), color=\"0.7\", alpha=0.2, label=\"prior\")\n",
    "ax1.plot(\n",
    "    xs, score(xs[:, np.newaxis]), \"--\", linewidth=1.0, color=\"black\", label=\"Score\"\n",
    ")\n",
    "\n",
    "ax1.plot(sample, score_sample, \"o\", markersize=6.0, label=\"Score eval.\")\n",
    "ax1.set_ylim(-0.3, 3.2)\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters are drawn from the current posterior approximation. The scores of these parameters are evaluated and added to the stack of parameters evaluated so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Infer estimation of score\n",
    "weights = get_weights_mc(prior, sample, n_sample_estim=10**5)\n",
    "m_score = np.sum(score_sample * weights)\n",
    "\n",
    "# For plottinf\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.set_ylabel(\"Density\")\n",
    "# ax1.set_ylabel(\"Score\")\n",
    "\n",
    "\n",
    "ax2.fill_between(xs, prior_norm.pdf(xs), color=\"0.7\", alpha=0.2, label=\"prior\")\n",
    "\n",
    "\n",
    "loc_sample = sample.copy()\n",
    "loc_weights = get_weights_mc(prior, loc_sample, n_sample_estim=10**6)\n",
    "\n",
    "sorter = np.argsort(loc_sample.flatten())\n",
    "loc_sample = loc_sample[sorter]\n",
    "loc_weights = loc_weights[sorter]\n",
    "\n",
    "cut = (loc_sample[1:] + loc_sample[:-1]) / 2\n",
    "\n",
    "loc_weights_renorm = loc_weights.copy()\n",
    "loc_weights_renorm[0] = 0.0\n",
    "loc_weights_renorm[-1] = 0.0\n",
    "\n",
    "loc_weights_renorm[1:-1] = loc_weights_renorm[1:-1] / (cut[1:, 0] - cut[:-1, 0])\n",
    "\n",
    "cut_plot = [xs[0]] + list(np.array([[a, a] for a in cut]).flatten()) + [xs[-1]]\n",
    "weights_for_plot = list(np.array([[a, a] for a in loc_weights_renorm]).flatten())\n",
    "ax2.plot(cut_plot, weights_for_plot, color=\"black\", label=\"Weights\", linewidth=1)\n",
    "\n",
    "ax2.plot(loc_sample, loc_weights_renorm, \"o\", color=\"tab:blue\", markersize=4)\n",
    "ax1.set_yticks([])\n",
    "fig.tight_layout()\n",
    "\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights are computed for each parameter in the stack of evaluated parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_s = gmap.T(sample)\n",
    "T_approx = exp_approximation(Ts=T_s, scores=score_sample, weights=weights)\n",
    "\n",
    "T_xs = gmap.T(xs[:, np.newaxis])\n",
    "score_approx = (T_xs * T_approx).sum(-1)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.set_ylabel(\"Density\")\n",
    "ax1.set_ylabel(\"Score\")\n",
    "\n",
    "ax2.fill_between(xs, prior_norm.pdf(xs), color=\"0.7\", alpha=0.2, label=\"prior\")\n",
    "ax1.plot(\n",
    "    xs, score(xs[:, np.newaxis]), \"--\", linewidth=1.0, color=\"black\", label=\"Score\"\n",
    ")\n",
    "\n",
    "ax1.plot(sample, score_sample, \"o\", markersize=9.0, label=\"Score eval.\")\n",
    "ax1.set_ylim(-0.3, 3.2)\n",
    "\n",
    "score_sample_approx = (T_s * T_approx).sum(-1)\n",
    "\n",
    "delta = np.sum(weights * (score_sample - score_sample_approx))\n",
    "\n",
    "ax1.plot(xs, delta + score_approx, color=\"peru\", linewidth=2.5, label=\"Score approx.\")\n",
    "fig.tight_layout()\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighing process is used to compute the best L2 approximation of the score of a given form (here for gaussians, quadratic forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = np.sum(weights * (score_sample - (T_s * T_approx).sum(-1)))\n",
    "\n",
    "T_prior = gmap.param_to_T(prior_param)\n",
    "T_updt_dir = -(temp**-1) * T_approx\n",
    "T_new = T_prior + (1 - dampen) * T_updt_dir\n",
    "\n",
    "post_param = gmap.T_to_param(T_new)\n",
    "\n",
    "post = gmap(post_param)\n",
    "post_norm = norm(loc=post.means[0], scale=np.sqrt(post.cov[0, 0]))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.set_ylabel(\"Density\")\n",
    "ax1.set_ylabel(\"Score\")\n",
    "\n",
    "ax2.fill_between(xs, prior_norm.pdf(xs), color=\"0.7\", alpha=0.2, label=\"prior\")\n",
    "\n",
    "ax1.set_ylim(-0.3, 3.2)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "ax1.plot(xs, delta + score_approx, color=\"peru\", linewidth=2.5, label=\"Score approx.\")\n",
    "fig.tight_layout()\n",
    "\n",
    "ax2.fill_between(\n",
    "    xs, post_norm.pdf(xs), color=\"skyblue\", alpha=0.2, label=\"post. approx.\"\n",
    ")\n",
    "ax2.set_ylabel(\"Density\")\n",
    "\n",
    "ax1.set_ylabel(\"Score\")\n",
    "\n",
    "fig.tight_layout()\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximated score is used to compute the posterior update.\n",
    "This learning cycle is then repeated until either convergence is achieved or the maximal number of optimisation steps is achieved.\n",
    "\n",
    "The algorithm is now showcased on a more complex function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba as nb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from surpbayes.bayes import iter_prior, variational_inference, iter_prior_vi\n",
    "from surpbayes.proba import GaussianMap, TensorizedGaussianMap\n",
    "\n",
    "# For plotting purposes\n",
    "from math import pi\n",
    "\n",
    "angles = np.linspace(0, 2.001 * pi, 1000)\n",
    "circle = np.array([np.cos(angles), np.sin(angles)])\n",
    "\n",
    "\n",
    "def half_cov(cov):\n",
    "    vals, vects = np.linalg.eigh(cov)\n",
    "    return (np.sqrt(vals) * vects) @ vects.T\n",
    "\n",
    "\n",
    "def repr_gauss(mean, cov, rad=1.0):\n",
    "    loc_circle = circle.copy()\n",
    "    return mean + rad * (half_cov(cov) @ loc_circle).T\n",
    "\n",
    "\n",
    "arr_1 = np.array([0.0, 1.0])\n",
    "arr_2 = np.array([1, -1])\n",
    "_shift = np.array([0.0, 0.5])\n",
    "\n",
    "\n",
    "def score(x):\n",
    "    z = x - _shift\n",
    "    return 4 * np.arctan(0.5 * ((z @ arr_1 - 1.0) ** 2 + 100.0 * (z @ arr_2) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surpbayes.proba import FactCovGaussianMap, FixedCovGaussianMap\n",
    "from time import time\n",
    "\n",
    "cov = np.array([[0.5, 0.4], [0.4, 0.5]])\n",
    "\n",
    "facgm = FactCovGaussianMap(2, cov=cov)\n",
    "ficgm = FixedCovGaussianMap(2, cov=cov)\n",
    "\n",
    "gmap = GaussianMap(2)\n",
    "\n",
    "tic = time()\n",
    "opt_res_new = variational_inference(\n",
    "    score,\n",
    "    gmap,\n",
    "    #     facgm,\n",
    "    temperature=0.1,  # the lambda term in the variational inference problem\n",
    "    optimizer=\"score_approx\",\n",
    "    per_step=96,\n",
    "    parallel=False,\n",
    "    vectorized=True,\n",
    "    print_rec=10,\n",
    "    chain_length=141,\n",
    "    n_estim_weights=10**5,\n",
    "    kl_max=0.04,\n",
    "    m_max=20,\n",
    "    xtol=10**-7,\n",
    "    kltol=10**-7,\n",
    "    silent=False,\n",
    "    dampen=0.1,\n",
    ")\n",
    "tac = time()\n",
    "print(\"Time elapsed:\", tac - tic)\n",
    "\n",
    "for i, param in enumerate(opt_res_new.hist_param[:]):\n",
    "    if i % 5 == 0:\n",
    "        proba = gmap(param)\n",
    "        proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "        plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)\n",
    "        plt.plot(proba.means[0], proba.means[1], \"x\")\n",
    "\n",
    "plt.title(\"Evolution of the posterior estimation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, param in enumerate(opt_res_new.hist_param[:]):\n",
    "    if i % 1 == 0:\n",
    "        proba = gmap(param)\n",
    "        proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "        #         xs, ys = shift(proba_repr[:, 0], proba_repr[:, 1])\n",
    "        xs, ys = proba_repr[:, 0], proba_repr[:, 1]\n",
    "        sns.lineplot(x=xs, y=ys, color=\"black\", sort=False, linewidth=0.2)\n",
    "#         plt.plot(proba.means[0], proba.means[1], \"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "x_min = -1.1\n",
    "x_max = 1.8\n",
    "n_x = 801\n",
    "\n",
    "y_min = -1.1\n",
    "y_max = 1.8\n",
    "n_y = 801\n",
    "\n",
    "x_axis_labels = np.linspace(\n",
    "    x_min, x_max, n_x\n",
    ")  # Avoid renormalisation issue at the angles\n",
    "y_axis_labels = np.linspace(y_min, y_max, n_y)\n",
    "\n",
    "\n",
    "def shift(xs, ys):\n",
    "    return n_x * (xs - x_min) / (x_max - x_min), n_y * (ys - y_min) / (y_max - y_min)\n",
    "\n",
    "\n",
    "values = np.array(np.meshgrid(y_axis_labels, x_axis_labels)).T\n",
    "\n",
    "z = score(values)\n",
    "alpha = np.linspace(0, 8, 100)\n",
    "\n",
    "sns.heatmap(\n",
    "    z.T,\n",
    "    xticklabels=y_axis_labels,\n",
    "    yticklabels=x_axis_labels,\n",
    "    cmap=sns.color_palette(\"Blues\", as_cmap=True),\n",
    ")\n",
    "# sns.heatmap(z)\n",
    "for i, param in enumerate(opt_res_new.hist_param):\n",
    "    if i % 2 == 0:\n",
    "        proba = gmap(param)\n",
    "        proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "        #         proba_repr  = repr_gauss(np.array([1.38,-1.1]), 0.01 * np.eye(2))\n",
    "\n",
    "        xs, ys = shift(proba_repr[:, 0], proba_repr[:, 1])\n",
    "        sns.lineplot(x=xs, y=ys, sort=False, color=\"yellow\", linewidth=1.0)\n",
    "#         plt.plot(proba.means[0], proba.means[1], \"x\")\n",
    "\n",
    "\n",
    "proba = gmap(opt_res_new.opti_param)\n",
    "proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "#         proba_repr  = repr_gauss(np.array([1.38,-1.1]), 0.01 * np.eye(2))\n",
    "\n",
    "xs, ys = shift(proba_repr[:, 0], proba_repr[:, 1])\n",
    "sns.lineplot(x=xs, y=ys, sort=False, color=\"red\", linewidth=1.0)\n",
    "\n",
    "# x,y = shift(0.0, 0.0)\n",
    "# sns.pointplot(x=x, y=y)\n",
    "\n",
    "# plt.title(\"A Copula\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.savefig(\"Rosenbrock0_01.png\", dpi=900, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.asfortranarray(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.random.normal(0, 1, 10**5).reshape((10, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(plot_score_evol)\n",
    "plot = plot_score_evol(opt_res_new, cmap=sns.color_palette(\"Blues\", as_cmap=True))\n",
    "plot.plot(opt_res_new.log_vi.means())\n",
    "plot.savefig(\"score_density_evolution.pdf\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surpbayes.bayes.plot.optim_result import plot_score_evol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_evol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = 0\n",
    "x_max = n_y\n",
    "\n",
    "z = x - x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift(xs, ys):\n",
    "    return n_x * (xs - x_min) / (x_max - x_min), n_y * (y_max - ys) / (y_max - y_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = gmap(opt_res_new.hist_param[0])\n",
    "proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "plt.plot(proba_repr[:, 0], proba_repr[:, 1], linewidth=1.0, label=\"prior\")\n",
    "\n",
    "for i, param in enumerate(opt_res_new.hist_param[:-5]):\n",
    "    if i % 1 == 0:\n",
    "        proba = gmap(param)\n",
    "        proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "        plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)\n",
    "#         plt.plot(proba.means[0], proba.means[1], \"x\")\n",
    "\n",
    "proba = gmap(opt_res_new.opti_param)\n",
    "proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "plt.plot(\n",
    "    proba_repr[:, 0],\n",
    "    proba_repr[:, 1],\n",
    "    linewidth=1.2,\n",
    "    color=\"crimson\",\n",
    "    label=\"posterior\",\n",
    ")\n",
    "plt.plot([1], [1], \"x\", c=\"k\", markersize=10, label=\"Glob. Min.\")\n",
    "\n",
    "plt.title(\"Evolution of the posterior estimation (score approx)\")\n",
    "plt.legend()\n",
    "plt.xticks([-1.0, 0.0, 1.0])\n",
    "plt.yticks([-1.0, 0.0, 1.0])\n",
    "# plt.savefig(\"score_approx_training_low_temp.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surpbayes.proba import FactCovGaussianMap, FixedCovGaussianMap\n",
    "from time import time\n",
    "\n",
    "cov = np.array([[0.5, 0.4], [0.4, 0.5]])\n",
    "\n",
    "facgm = FactCovGaussianMap(2, cov=cov)\n",
    "ficgm = FixedCovGaussianMap(2, cov=cov)\n",
    "\n",
    "tic = time()\n",
    "opt_res_new = variational_inference(\n",
    "    score,\n",
    "    facgm,\n",
    "    temperature=0.01,  # the lambda term in the variational inference problem\n",
    "    optimizer=\"score_approx\",\n",
    "    per_step=320,\n",
    "    parallel=False,\n",
    "    vectorized=True,\n",
    "    print_rec=10,\n",
    "    chain_length=61,\n",
    "    n_estim_weights=10**5,\n",
    "    kl_max=0.05,\n",
    "    m_max=20,\n",
    "    xtol=10**-9,\n",
    "    #     alpha_filter=0.99,\n",
    "    silent=False,\n",
    "    dampen=0.5,\n",
    ")\n",
    "tac = time()\n",
    "print(tac - tic)\n",
    "for i, param in enumerate(opt_res_new.hist_param):\n",
    "    if i % 1 == 0:\n",
    "        proba = facgm(param)\n",
    "        proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "        plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)\n",
    "        plt.plot(proba.means[0], proba.means[1], \"x\")\n",
    "\n",
    "plt.title(\"Evolution of the posterior estimation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from surpbayes.bayes.plot.score_approx import plot_weights_per_gen\n",
    "\n",
    "plot_weights_per_gen(opt_res_new, n_sample_estim_weight=10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surpbayes.bayes.plot.optim_result import plot_score_evol\n",
    "\n",
    "plot = plot_score_evol(opt_res_new)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surpbayes.bayes.plot import plot_hist_vi, plot_scores\n",
    "\n",
    "plot = plot_scores(sample_val=opt_res_new.sample_val, marker=\"x\", s=0.03)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surpbayes.proba import FactCovGaussianMap, FixedCovGaussianMap\n",
    "\n",
    "cov = np.array([[0.5, 0.4], [0.4, 0.5]])\n",
    "\n",
    "facgm = FactCovGaussianMap(2, cov=cov)\n",
    "ficgm = FixedCovGaussianMap(2, cov=cov)\n",
    "\n",
    "opt_res_new = variational_inference(\n",
    "    score,\n",
    "    facgm,\n",
    "    temperature=0.1,  # the lambda term in the variational inference problem\n",
    "    optimizer=\"score_approx\",\n",
    "    per_step=160,\n",
    "    parallel=False,\n",
    "    vectorized=True,\n",
    "    print_rec=10,\n",
    "    chain_length=41,\n",
    "    n_estim_weights=10**5,\n",
    "    kl_max=10**-2,\n",
    "    kltol=0.0,\n",
    "    m_max=20,\n",
    "    xtol=10**-6,\n",
    "    silent=False,\n",
    "    dampen=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surpbayes.bayes.plot import plot_score_push_begin_end, plot_score_evol\n",
    "\n",
    "plot = plot_score_evol(opt_res_new)\n",
    "plot.show()\n",
    "plot.clf()\n",
    "plot = plot_score_push_begin_end(opt_res_new)\n",
    "plot.show()\n",
    "plot.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sample_val = opt_res_new.sample_val\n",
    "sns.kdeplot(sample_val.vals()[sample_val.gen_tracker() == 0])\n",
    "sns.kdeplot(\n",
    "    sample_val.vals()[sample_val.gen_tracker() == np.max(sample_val.gen_tracker())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the space of probability distributions on which we wish to optimize. Here we consider a score defined on a two dimensional space, and therefore use gaussian distributions on $\\mathbb{R}^2$. The prior will be the standard distribution\n",
    "\n",
    "## Gradient based algorithm\n",
    "\n",
    "To use the gradient based routine, the parameter 'VI_method' must be either \"corr_weights\" or \"knn\". It is advised to use 'corr_weights'.\n",
    "It is normal behavior that the optimisation procedure raises some ProbaBadGrad warnings. These indicate that a problematic gradient estimation was rejected as it damaged significantly the score. No need to worry about those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_map = GaussianMap(2)\n",
    "\n",
    "# We define the prior as the reference gaussian distribution, i.e. N(0,Id)\n",
    "prior_param = gauss_map.ref_param\n",
    "\n",
    "# To solve the variational inference problem, we use the variational_inference function.\n",
    "opt_res_grad = variational_inference(\n",
    "    score,\n",
    "    gauss_map,\n",
    "    prior_param=prior_param,\n",
    "    temperature=0.1,  # the lambda term in the variational inference problem\n",
    "    per_step=160,\n",
    "    optimizer=\"corr_weights\",\n",
    "    gen_decay=np.log(1.2),\n",
    "    k=160 * 20,\n",
    "    parallel=False,\n",
    "    vectorized=True,\n",
    "    print_rec=50,\n",
    "    chain_length=501,\n",
    "    refuse_conf=0.9,\n",
    "    momentum=0.9,\n",
    "    eta=0.05,\n",
    "    silent=False,\n",
    ")\n",
    "\n",
    "# It is normal behavior that the optimisation procedure raises some ProbaBadGrad warnings.\n",
    "# These indicate that a problematic gradient estimation was rejected as it damaged significantly\n",
    "# the score. No need to worry about those.\n",
    "\n",
    "# We can access the parameter describing the posterior through the opti_param attribute\n",
    "post_param = opt_res_grad.opti_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = gmap(opt_res_grad.hist_param[0])\n",
    "proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "plt.plot(proba_repr[:, 0], proba_repr[:, 1], linewidth=1.0, label=\"prior\")\n",
    "\n",
    "for i, param in enumerate(opt_res_grad.hist_param[:]):\n",
    "    if i % 3 == 0:\n",
    "        proba = gmap(param)\n",
    "        proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "        plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)\n",
    "#         plt.plot(proba.means[0], proba.means[1], \"x\")\n",
    "\n",
    "proba = gmap(opt_res_grad.opti_param)\n",
    "proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "plt.plot(\n",
    "    proba_repr[:, 0],\n",
    "    proba_repr[:, 1],\n",
    "    linewidth=1.2,\n",
    "    color=\"crimson\",\n",
    "    label=\"posterior\",\n",
    ")\n",
    "plt.plot([1], [1], \"x\", c=\"0.0\", markersize=10, label=\"Glob. Min.\")\n",
    "\n",
    "plt.title(\"Evolution of the posterior estimation (corr weights)\")\n",
    "plt.legend()\n",
    "plt.xticks([-1.0, 0.0, 1.0])\n",
    "plt.yticks([-1.0, 0.0, 1.0])\n",
    "plt.savefig(\"corr_weights_training.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# for i, param in enumerate(opt_res_grad.hist_param[:-450]):\n",
    "#     if i % 2 == 0:\n",
    "#         proba = gauss_map(param)\n",
    "#         proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "#         plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimisation start by modification of the covariance\n",
    "\n",
    "for i, param in enumerate(opt_res_grad.hist_param[:25:2]):\n",
    "    if i % 1 == 0:\n",
    "        proba = gauss_map(param)\n",
    "        proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "        plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution then shifts towards the correct mean value\n",
    "for i, param in enumerate(opt_res_grad.hist_param[25:500:30]):\n",
    "    if i % 1 == 0:\n",
    "        proba = gauss_map(param)\n",
    "        proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "        plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evolution of the VI score can also be tracked:\n",
    "plt.plot(opt_res_grad.hist_score)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_push_begin_end(opt_res_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score approximation\n",
    "\n",
    "Score approximation is now the default way to optimise Catoni's bound for Gaussian, BlockDiagonalGaussian and ExponentialFamily distribution map. These yield much more stable result, but have more processing time between steps. The number of calls to the model is greatly reduced, and the results are more accurate, especially when the temperature is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surpbayes.proba import GaussianMap\n",
    "\n",
    "gauss_map = GaussianMap(2)\n",
    "opt_res_sa = variational_inference(\n",
    "    score,\n",
    "    gauss_map,\n",
    "    # prior_param=prior_param,\n",
    "    temperature=0.05,  # the lambda term in the variational inference problem\n",
    "    optimizer=\"score_approx\",\n",
    "    per_step=160,\n",
    "    parallel=False,\n",
    "    vectorized=True,\n",
    "    print_rec=1,\n",
    "    chain_length=51,\n",
    "    n_estim_weights=10**4,\n",
    "    kl_max=0.1,\n",
    "    m_max=20,\n",
    "    xtol=10**-6,\n",
    "    alpha_filter=0.9,\n",
    "    silent=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = gauss_map(opt_res_sa.opti_param)\n",
    "proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "plt.plot(\n",
    "    proba_repr[:, 0],\n",
    "    proba_repr[:, 1],\n",
    "    color=\"blue\",\n",
    "    linewidth=0.2,\n",
    "    label=\"Approx. Score\",\n",
    ")\n",
    "\n",
    "proba = gauss_map(opt_res_grad.opti_param)\n",
    "proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "plt.plot(\n",
    "    proba_repr[:, 0],\n",
    "    proba_repr[:, 1],\n",
    "    color=\"black\",\n",
    "    linewidth=0.2,\n",
    "    label=\"Gradient Descent\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.title(\"Result of optimisation routines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences are significative. Both distributions exhibit high correlations. The score approximation routine migrated towards the minimum value $(1,1)$, while the gradient descent algorithm ended its migtration too early on.\n",
    "\n",
    "We plot the successive distributions to exhibiti the improved stability of the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, param in enumerate(opt_res_sa.hist_param):\n",
    "    proba = gauss_map(param)\n",
    "    proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "    plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)\n",
    "plt.title(\"Evolution of the posterior estimation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score now decreases in a much more regular fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(opt_res_sa.hist_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks for other distribution maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surpbayes.proba import FactCovGaussianMap, FixedCovGaussianMap\n",
    "\n",
    "cov = np.array([[1, 0.5], [0.5, 1]])\n",
    "\n",
    "facgm = FactCovGaussianMap(2, cov=cov)\n",
    "ficgm = FixedCovGaussianMap(2, cov=cov)\n",
    "\n",
    "opt_res = variational_inference(\n",
    "    score,\n",
    "    facgm,\n",
    "    temperature=0.001,  # the lambda term in the variational inference problem\n",
    "    optimizer=\"score_approx\",\n",
    "    per_step=320,\n",
    "    parallel=False,\n",
    "    vectorized=True,\n",
    "    print_rec=10,\n",
    "    chain_length=51,\n",
    "    n_estim_weights=10**4,\n",
    "    kl_max=0.1,\n",
    "    m_max=20,\n",
    "    xtol=10**-6,\n",
    "    alpha_filter=0.9,\n",
    "    silent=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setting above was a special case, where the true score was quadratic. As the score_approx algorithm for gaussians relies on quadratic approximation, this heavily favors this approach. We investigate a case where the approximations looked for (quadratic with diagonal matrix) does not fit the true score (quadratic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, param in enumerate(opt_res.hist_param[:5]):\n",
    "    if i % 1 == 0:\n",
    "        proba = facgm(param)\n",
    "        proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "        plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)\n",
    "        plt.plot(proba.means[0], proba.means[1], \"x\")\n",
    "\n",
    "plt.title(\"Evolution of the posterior estimation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set corr between -1 and 1. The higher the absolute value of corr, the harder the problem is.\n",
    "corr = 0.7\n",
    "\n",
    "\n",
    "def make_mat(vars, corr=0.0):\n",
    "    assert np.abs(corr) < 1.0\n",
    "    anti_diag = np.sqrt(vars[0] * vars[1]) * corr\n",
    "    return np.array([[vars[0], anti_diag], [anti_diag, vars[1]]])\n",
    "\n",
    "\n",
    "center = np.array([-1.0, 1.0])\n",
    "mat = make_mat([4.0, 1.0], corr)\n",
    "\n",
    "\n",
    "def score(xs):\n",
    "    return (((xs - center) @ mat) * (xs - center)).sum(-1)\n",
    "\n",
    "\n",
    "from surpbayes.proba import BlockDiagGaussMap\n",
    "\n",
    "bgmap = BlockDiagGaussMap([[1], [0]])\n",
    "\n",
    "opt_res = variational_inference(\n",
    "    score,\n",
    "    bgmap,\n",
    "    temperature=0.2,\n",
    "    per_step=320,\n",
    "    kl_max=0.5,\n",
    "    chain_length=30,\n",
    "    dampen=0.0,\n",
    "    alpha_filter=0.95,\n",
    "    n_estim_weights=3 * 10**5,\n",
    "    vectorized=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount of reuse of the previous evaluations\n",
    "The whole point of the score_approx technique is to make most use of all previous evaluations of the score. We can track the impact these previous evaluations have by checking the weight given to each sample at the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surpbayes.bayes.plot import plot_weight_per_gen\n",
    "\n",
    "plot_weight_per_gen(opt_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surpbayes.bayes.score_approx.weighing.monte_carlo import get_weights_mc_gauss\n",
    "\n",
    "weights = get_weights_mc_gauss(\n",
    "    bgmap(opt_res.opti_param), opt_res.sample_val.sample(), n_sample_estim=10**6\n",
    ")\n",
    "plt.plot(weights)\n",
    "plt.show()\n",
    "weights_per_gen = weights.reshape((30, 320)).sum(1)\n",
    "plt.plot(weights_per_gen, label=\"Tot weight per generation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the first 5 generations have smaller impact, all generations after generation 8 have non negligible weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[bgmap.kl(par1, par0) for par1, par0 in zip(opt_res.hist_param[1:], opt_res.hist_param)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution then shifts towards the correct mean value\n",
    "for i, param in enumerate(opt_res.hist_param):\n",
    "    proba = bgmap(param)\n",
    "    proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "    plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.plot(opt_res.hist_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, variational_inference can redirect to two routines (VI_method argument): either \"corr_weights\" or \"KNN\". The name refers to the method used in order to make most use of the evaluations to the score function.\n",
    "\n",
    "The 'variational_inference' function was designed for situations where evaluating the 'score' is rather expensive. It is still, however, an accelarated gradient descent algorithm. The change is that the gradient's expression involves an expectation with respect to the current distribution. The naïve approach consisting in sampling iid samples from the current distribution to obtain an unbiased estimation of the expectation is improved upon by recycling previous samples. These are generated from distributions similar to the current one, if small optimization steps are done ('eta' parameter is small).\n",
    "\n",
    "As it is not possible to use these samples directly, two procedures are proposed. \"corr_weights\" consists in giving each sample a weight to adjust for the difference of probability for it being drawn between the current and previous distributions. \"KNN\" consists in constructing a surrogate score using a K-Nearest neighbor algorithm, then using this surrogate on a large number of samples to compute the derivative.\n",
    "\n",
    "The number of samples used all in all when evaluating the derivative is controlled by the argument 'k'. By default it is None, amounting to all samples being used.\n",
    "\n",
    "For \"corr_weights\", it is possible and advisable to set the 'gen_decay' parameter higher than 0 (default value). The 'gen_decay' parameter gives a decreasing weights to older generations when computing the derivative. While generations just before tend to be close to the current one, older ones would no longer be representative, and could have a negative impact when computing the derivative. The higher 'gen_decay', the lower will be the influence of older generation (exponentially decreasing weights of $\\exp(-t \\times gen\\_decay)$ are used).\n",
    "\n",
    "For \"KNN\", the number of neighbors used by the K-nearest neighbors algorithm is NOT controlled by the argument 'k', but by \"n_neighbors\". As stated above, \"k\" controls the number of samples used. By default, \"n_neighbors\" is 5.\n",
    "\n",
    "\n",
    "The 'corr_weights' method has the edge in most cases. For instance, 'KNN' by design does not like situations where the Hessian near the minima has eigenvalues of different magnitudes, which is the case for the Rosenbrock function tested here. This could be improved upon by learning the distance used in 'KNN', or by training different surrogates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison, variational_inference with KNN method\n",
    "\n",
    "opt_res = variational_inference(\n",
    "    score,\n",
    "    gauss_map,\n",
    "    prior_param=prior_param,\n",
    "    temperature=0.1,\n",
    "    per_step=1600,\n",
    "    optimizer=\"knn\",\n",
    "    k=None,\n",
    "    parallel=False,\n",
    "    print_rec=20,\n",
    "    chain_length=600,\n",
    "    vectorized=True,\n",
    "    momentum=0.99,\n",
    "    eta=0.1,\n",
    "    silent=True,\n",
    ")\n",
    "\n",
    "end_proba = gauss_map(opt_res.opti_param)\n",
    "\n",
    "print(\n",
    "    f\"Mean score of estimated posterior: {end_proba.integrate(score, n_sample = 1000)}\"\n",
    ")\n",
    "\n",
    "# The evolution of the VI score can also be tracked:\n",
    "plt.plot(opt_res.hist_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iter prior procedure\n",
    "\n",
    "The iterated prior procedure is not a Bayesian technique at all. It is actually an optimisation routine, using a Bayesian flavored technique.\n",
    "\n",
    "The goal is minimizing $S(x)$, a score function. In order to do that, parameters are drawn from a distribution. The distribution for the next generation is then obtained by centering around the best parameter found so far, and by using the top parameters found so far to construct the covariance. Each dimension of the parameter is drawn independantly from a gaussian distribution, so that the covariance is diagonal and can be defined by using the empirical standard deviations of the top parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The initial prior_param is a parameter for the TensorizedGaussianMap.\n",
    "ini_prior = np.zeros((2, 2))\n",
    "ini_prior[1] = 1.0\n",
    "\n",
    "opt_res = iter_prior(\n",
    "    score,\n",
    "    ini_prior_param=ini_prior,\n",
    "    gen_per_step=800,\n",
    "    chain_length=50,\n",
    "    keep=100,\n",
    "    frac_sparse=0.0,\n",
    "    parallel=False,\n",
    ")\n",
    "\n",
    "# The opti_param attribute of opt_res gives a distribution and NOT a parameter\n",
    "opti_proba_param = opt_res.opti_param\n",
    "\n",
    "# The optimal parameter can still be found:\n",
    "opti_param = opt_res.full_sample[0]\n",
    "print(opti_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The technique used in iter prior can still be useful in the context of variational inference, in order to construct quickly a good initial distribution. The function iter_prior_vi is designed precisely for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_res = iter_prior_vi(\n",
    "    score,\n",
    "    prior_param=ini_prior,\n",
    "    temperature=0.1,\n",
    "    gen_per_step=800,\n",
    "    chain_length=50,\n",
    "    keep=100,\n",
    "    frac_sparse=0.0,\n",
    "    parallel=False,\n",
    "    vectorized=True,\n",
    ")\n",
    "\n",
    "# The opti_param attribute of opt_res gives a distribution and NOT a parameter\n",
    "opti_proba_param = opt_res.opti_param\n",
    "\n",
    "start_post = np.zeros((3, 2))\n",
    "\n",
    "start_post[0] = opti_proba_param[0]\n",
    "start_post[1:] = np.diag(opti_proba_param[1])\n",
    "\n",
    "opt_res = variational_inference(\n",
    "    score,\n",
    "    gauss_map,\n",
    "    prior_param=prior_param,\n",
    "    post_param=start_post,\n",
    "    temperature=0.1,\n",
    "    per_step=160,\n",
    "    VI_method=\"corr_weights\",\n",
    "    gen_decay=np.log(1.2),\n",
    "    k=160 * 20,\n",
    "    parallel=False,\n",
    "    vectorized=True,\n",
    "    print_rec=2,\n",
    "    chain_length=50,\n",
    "    refuse_conf=0.95,\n",
    "    momentum=0.95,\n",
    "    eta=0.1,\n",
    "    silent=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(opt_res.hist_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform priors - Gaussian computations\n",
    "\n",
    "The proba module offers a class of distributions on the hypercube benefitting from Gaussian like interpretation when the distribution are sufficiently concentrated and exact computations for KL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surpbayes.proba.gauss import GaussHypercubeMap\n",
    "\n",
    "dim = 2\n",
    "\n",
    "# Toy score function\n",
    "def score(x):\n",
    "    return np.arctan(\n",
    "        0.2 * (x @ np.array([1.0, 0.0], dtype=np.float64) - 0.6) ** 2\n",
    "        + 20 * (x @ np.array([1.0, -2.0], dtype=np.float64)) ** 2\n",
    "    )\n",
    "\n",
    "\n",
    "pmap = GaussHypercubeMap(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_res = variational_inference(\n",
    "    score,\n",
    "    pmap,\n",
    "    temperature=0.01,  # the lambda term in the variational inference problem\n",
    "    per_step=80,\n",
    "    dampen=0.1,\n",
    "    kl_max=0.2,\n",
    "    parallel=False,\n",
    "    vectorized=True,\n",
    "    print_rec=10,\n",
    "    chain_length=51,\n",
    "    silent=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior can adapt to scores with strong identifiability issues such as Rosenbrock, since the probabilities can exhibit strong correlation structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "proba = pmap(opt_res.opti_param)\n",
    "# The log density of the function can be accessed through log_dens\n",
    "x_axis_labels = np.linspace(10**-4, 1 - 10**-4, 121)\n",
    "y_axis_labels = np.linspace(10**-4, 1 - 10**-4, 121)\n",
    "\n",
    "values = np.array(np.meshgrid(y_axis_labels, x_axis_labels)).T\n",
    "z = proba.dens(values)\n",
    "\n",
    "sns.heatmap(z, xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "plt.title(\"Posterior distribution\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(opt_res.hist_score, label=\"Evolution of the VI score\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tests for problems of larger dimensions\n",
    "\n",
    "d = 80\n",
    "k = 5\n",
    "\n",
    "blocks = [list(range(i * k, min(d, (i + 1) * k))) for i in range(int(np.ceil(d / k)))]\n",
    "\n",
    "mat = (np.random.uniform(0.5, 2, d)) * np.random.normal(0, 1, (4 * d, d))\n",
    "mat = mat.T @ mat / (4 * d)\n",
    "\n",
    "print(np.linalg.eigvalsh(mat))\n",
    "\n",
    "center = np.random.normal(0, 1, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(xs):\n",
    "    deltas = np.arctan(xs - center)\n",
    "    return (deltas * (deltas @ mat)).sum(-1)\n",
    "\n",
    "\n",
    "bgmap = BlockDiagGaussMap(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_res = variational_inference(\n",
    "    score,\n",
    "    bgmap,\n",
    "    per_step=800,\n",
    "    kl_max=3.0,\n",
    "    temperature=0.1,\n",
    "    chain_length=20,\n",
    "    vectorized=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[bgmap.kl(par1, par0) for par1, par0 in zip(opt_res.hist_param[1:], opt_res.hist_param)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(opt_res.hist_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(x):\n",
    "    return -(\n",
    "        1.2 * np.exp(-4 * (x @ [1] - 4) ** 2) + 1.0 * np.exp(-1.0 * (x @ [1]) ** 2)\n",
    "    )\n",
    "\n",
    "\n",
    "xs = np.linspace(-4, 8, 4000).reshape((4000, 1))\n",
    "\n",
    "plt.plot(xs, score(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surpbayes.accu_xy import AccuSampleVal\n",
    "\n",
    "xs = np.linspace(3.5, 4.5, 1000).reshape((1000, 1))\n",
    "accu = AccuSampleVal((1,), 1000)\n",
    "accu.add(xs, score(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * (1 - norm.cdf(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "gmap = GaussianMap(1)\n",
    "\n",
    "tic = time()\n",
    "opt_res_stable = variational_inference(\n",
    "    score,\n",
    "    gmap,\n",
    "    #     facgm,\n",
    "    #     prev_eval=accu, # Check if this improves matter or not\n",
    "    temperature=0.002,  # the lambda term in the variational inference problem\n",
    "    VI_method=\"score_approx\",\n",
    "    prior_param=np.array([[0.0], [2.0]]),\n",
    "    post_param=np.array([[4.0], [1.0]]),\n",
    "    per_step=100,\n",
    "    parallel=True,\n",
    "    vectorized=False,\n",
    "    print_rec=10,\n",
    "    chain_length=101,\n",
    "    n_estim_weights=10**6,\n",
    "    kl_max=0.1,\n",
    "    m_max=20,\n",
    "    xtol=10**-8,\n",
    "    kltol=10**-8,\n",
    "    alpha_filter=1.0,\n",
    "    silent=False,\n",
    "    dampen=0.01,\n",
    ")\n",
    "tac = time()\n",
    "print(\"Time elapsed:\", tac - tic)\n",
    "\n",
    "xs = np.linspace(-4, 6, 1000)\n",
    "\n",
    "for i, param in enumerate(opt_res_stable.hist_param[:]):\n",
    "    if i % 2 == 0:\n",
    "        proba = gmap(param)\n",
    "        plt.plot(\n",
    "            xs,\n",
    "            norm(proba.means[0], np.sqrt(proba.cov[0])).pdf(xs),\n",
    "            color=\"black\",\n",
    "            linewidth=0.4,\n",
    "        )\n",
    "#         proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "#         plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)\n",
    "#         plt.plot(proba.means[0], proba.means[1], \"x\")\n",
    "\n",
    "plt.title(\"Evolution of the posterior estimation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_res_stable.opti_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, param in enumerate(opt_res_stable.hist_param[10:30]):\n",
    "    if i % 1 == 0:\n",
    "        proba = gmap(param)\n",
    "        plt.plot(\n",
    "            xs,\n",
    "            norm(proba.means[0], np.sqrt(proba.cov[0])).pdf(xs),\n",
    "            color=\"black\",\n",
    "            linewidth=0.4,\n",
    "        )\n",
    "#         proba_repr = repr_gauss(proba.means, proba.cov)\n",
    "#         plt.plot(proba_repr[:, 0], proba_repr[:, 1], color=\"black\", linewidth=0.2)\n",
    "#         plt.plot(proba.means[0], proba.means[1], \"x\")\n",
    "\n",
    "plt.title(\"Evolution of the posterior estimation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spbb-env",
   "language": "python",
   "name": "spbb-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "2fa9d9f029224ed00a1f4f48f176862f4f412effc3d402c8e89d290752261f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
